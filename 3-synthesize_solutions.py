#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èµ›åŠ›æ–¯/é—®ç•Œ èˆ†æƒ…åˆ†æç»“æœç»¼åˆè§£å†³æ–¹æ¡ˆç”Ÿæˆå·¥å…·
ä»å¤šä¸ªåˆ†æç»“æœæ–‡ä»¶ä¸­æå–é¢„è­¦å’Œé«˜å±é—®é¢˜ï¼Œé€šè¿‡AIç”Ÿæˆç»¼åˆè§£å†³æ–¹æ¡ˆ
"""

import json
import os
import glob
from collections import Counter
from datetime import datetime
from openai import OpenAI
from typing import Dict, List, Optional
from dotenv import load_dotenv
from json_repair import repair_json

# ==================== åŠ è½½ç¯å¢ƒå˜é‡ ====================
load_dotenv()

# ==================== é…ç½® ====================
API_BASE_URL = os.environ.get("API_BASE_URL", "https://api.tu-zi.com/v1")
MODEL_NAME = os.environ.get("MODEL_NAME", "claude-sonnet-4-5-20250929")
API_KEY = os.environ.get("API_KEY", "")

# ==================== åˆå§‹åŒ–å®¢æˆ·ç«¯ ====================
client = OpenAI(
    base_url=API_BASE_URL,
    api_key=API_KEY
)

# ==================== æ‰«æåˆ†æç»“æœæ–‡ä»¶ ====================
def scan_analysis_files(analysis_dir: str = "analysis_results", index_file: str = "analysis_results/files_index.json") -> List[str]:
    """ä»ç´¢å¼•æ–‡ä»¶ä¸­è¯»å–éœ€è¦åˆ†æçš„æ–‡ä»¶åˆ—è¡¨"""
    
    # é¦–å…ˆæ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(index_file):
        print(f"âš ï¸  ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_file}")
        print(f"   å›é€€åˆ°æ‰«æç›®å½•æ¨¡å¼...")
        # å›é€€åˆ°åŸæ¥çš„æ‰«ææ–¹å¼
        if not os.path.exists(analysis_dir):
            print(f"âš ï¸  åˆ†æç»“æœç›®å½•ä¸å­˜åœ¨: {analysis_dir}")
            return []
        
        pattern = os.path.join(analysis_dir, "*.json")
        files = glob.glob(pattern)
        files = [f for f in files if not f.endswith("files_index.json")]
        files.sort()
        return files
    
    # è¯»å–ç´¢å¼•æ–‡ä»¶
    try:
        with open(index_file, 'r', encoding='utf-8') as f:
            index_data = json.load(f)
        
        file_list = index_data.get("files", [])
        
        if not file_list:
            print(f"âš ï¸  ç´¢å¼•æ–‡ä»¶ä¸­æ²¡æœ‰æ–‡ä»¶åˆ—è¡¨")
            return []
        
        # æ„å»ºå®Œæ•´è·¯å¾„
        files = []
        for filename in file_list:
            file_path = os.path.join(analysis_dir, filename)
            if os.path.exists(file_path):
                files.append(file_path)
            else:
                print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡: {filename}")
        
        files.sort()
        print(f"âœ“ ä»ç´¢å¼•æ–‡ä»¶è¯»å–åˆ° {len(files)} ä¸ªæ–‡ä»¶")
        return files
        
    except Exception as e:
        print(f"âœ— è¯»å–ç´¢å¼•æ–‡ä»¶å¤±è´¥ {index_file}: {e}")
        print(f"   å›é€€åˆ°æ‰«æç›®å½•æ¨¡å¼...")
        # å›é€€åˆ°åŸæ¥çš„æ‰«ææ–¹å¼
        if not os.path.exists(analysis_dir):
            print(f"âš ï¸  åˆ†æç»“æœç›®å½•ä¸å­˜åœ¨: {analysis_dir}")
            return []
        
        pattern = os.path.join(analysis_dir, "*.json")
        files = glob.glob(pattern)
        files = [f for f in files if not f.endswith("files_index.json")]
        files.sort()
        return files

# ==================== åŠ è½½åˆ†æç»“æœ ====================
def load_analysis_data(file_path: str) -> List[Dict]:
    """åŠ è½½å•ä¸ªåˆ†æç»“æœæ–‡ä»¶"""
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        print(f"âœ“ åŠ è½½æ–‡ä»¶: {os.path.basename(file_path)} ({len(data)} æ¡æ•°æ®)")
        return data
    except Exception as e:
        print(f"âœ— åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return []

# ==================== åŠ è½½GEOæ–¹æ³•è®ºæ–‡ä»¶ ====================
def load_geo_methodology(method_file: str = "ref_md/GEOæ–¹æ³•è®ºä¸å®æˆ˜å…¨æ¡ˆ.md") -> str:
    """åŠ è½½GEOæ–¹æ³•è®ºæ–‡ä»¶å†…å®¹"""
    
    try:
        if not os.path.exists(method_file):
            print(f"âš ï¸  GEOæ–¹æ³•è®ºæ–‡ä»¶ä¸å­˜åœ¨: {method_file}")
            return ""
        
        with open(method_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"âœ“ å·²åŠ è½½GEOæ–¹æ³•è®ºæ–‡ä»¶: {method_file}")
        return content
    except Exception as e:
        print(f"âœ— åŠ è½½GEOæ–¹æ³•è®ºæ–‡ä»¶å¤±è´¥ {method_file}: {e}")
        return ""

# ==================== æå–å¹³å°ä¿¡æ¯ ====================
def extract_platforms(critical_issues: Dict[str, List[Dict]]) -> str:
    """
    ä»å…³é”®é—®é¢˜ä¸­æå–å¹³å°ä¿¡æ¯
    è¿”å›æœ€å¸¸è§çš„å¹³å°ï¼Œå¦‚æœæœ‰å¤šä¸ªå¹³å°åˆ™è¿”å›å¹³å°åˆ—è¡¨
    """
    all_platforms = []
    for category in ["é«˜å±", "é¢„è­¦"]:
        for item in critical_issues[category]:
            platform = item.get("Platform", "").strip()
            if platform:
                # ç»Ÿä¸€å¹³å°åç§°ï¼ˆå¤„ç†å¤§å°å†™ä¸ä¸€è‡´ï¼‰
                platform_normalized = platform
                if platform.lower() == "deepseek":
                    platform_normalized = "DeepSeek"
                all_platforms.append(platform_normalized)
    
    if not all_platforms:
        return "å¤šä¸ªAIå¹³å°"
    
    # ç»Ÿè®¡å¹³å°å‡ºç°æ¬¡æ•°
    platform_counter = Counter(all_platforms)
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªå¹³å°æˆ–æŸä¸ªå¹³å°å ä¸»å¯¼ï¼ˆ>70%ï¼‰ï¼Œè¿”å›è¯¥å¹³å°
    most_common_platform, count = platform_counter.most_common(1)[0]
    total_count = len(all_platforms)
    
    if count / total_count > 0.7:
        return most_common_platform
    else:
        # å¤šä¸ªå¹³å°ï¼Œè¿”å›å‰3ä¸ªæœ€å¸¸è§çš„å¹³å°
        top_platforms = [p for p, _ in platform_counter.most_common(3)]
        return "ã€".join(top_platforms)

# ==================== æå–é¢„è­¦å’Œé«˜å±å†…å®¹ ====================
def extract_critical_issues(all_data: List[Dict]) -> Dict[str, List[Dict]]:
    """
    æå–æ‰€æœ‰é¢„è­¦(ğŸŸ¡)å’Œé«˜å±(ğŸ”´)çš„åˆ†æç»“æœ
    æŒ‰å®‰å…¨çŠ¶æ€åˆ†ç±»
    """
    
    critical_issues = {
        "é«˜å±": [],
        "é¢„è­¦": []
    }
    
    for item in all_data:
        security_status = item.get("Security_Status", "")
        
        if "ğŸ”´" in security_status or "é«˜å±" in security_status:
            critical_issues["é«˜å±"].append(item)
        elif "ğŸŸ¡" in security_status or "é¢„è­¦" in security_status:
            critical_issues["é¢„è­¦"].append(item)
    
    return critical_issues

# ==================== æ„å»ºç»¼åˆåˆ†ææç¤ºè¯ ====================
def build_synthesis_prompt(critical_issues: Dict[str, List[Dict]], method_content: str, platform: str) -> str:
    """æ„å»ºç”¨äºç”Ÿæˆç»¼åˆè§£å†³æ–¹æ¡ˆçš„æç¤ºè¯"""
    
    # ç»Ÿè®¡ä¿¡æ¯
    high_risk_count = len(critical_issues["é«˜å±"])
    warning_count = len(critical_issues["é¢„è­¦"])
    
    # å‡†å¤‡é«˜å±é—®é¢˜æ‘˜è¦
    high_risk_summary = []
    for idx, item in enumerate(critical_issues["é«˜å±"][:10], 1):  # æœ€å¤šå±•ç¤º10ä¸ª
        high_risk_summary.append(f"""
ã€é«˜å±é—®é¢˜ {idx}ã€‘
- å¹³å°: {item.get('Platform', 'N/A')}
- ç”¨æˆ·æé—®: {item.get('User_Query', 'N/A')}
- é£é™©è¯Šæ–­: {item.get('Risk_Diagnosis', 'N/A')[:200]}...
- ç­–ç•¥å»ºè®®: {item.get('Strategy_Action', 'N/A')[:300]}...
""")
    
    # å‡†å¤‡é¢„è­¦é—®é¢˜æ‘˜è¦
    warning_summary = []
    for idx, item in enumerate(critical_issues["é¢„è­¦"][:15], 1):  # æœ€å¤šå±•ç¤º15ä¸ª
        warning_summary.append(f"""
ã€é¢„è­¦é—®é¢˜ {idx}ã€‘
- å¹³å°: {item.get('Platform', 'N/A')}
- ç”¨æˆ·æé—®: {item.get('User_Query', 'N/A')}
- é£é™©è¯Šæ–­: {item.get('Risk_Diagnosis', 'N/A')[:200]}...
- å“ç‰Œå°è±¡è¯„åˆ†: {item.get('Brand_Impression', 'N/A')[:100]}...
""")
    
    prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„GEO (Generative Engine Optimizationï¼Œç”Ÿæˆå¼å¼•æ“ä¼˜åŒ–) ä¸“å®¶å’ŒAIå†…å®¹ç”Ÿæ€æ²»ç†é¡¾é—®ï¼Œä¸“æ³¨äºæ–°èƒ½æºæ±½è½¦è¡Œä¸šçš„å“ç‰Œå£°èª‰ç®¡ç†ã€‚

# ä»»åŠ¡èƒŒæ™¯

æˆ‘ä»¬å¯¹èµ›åŠ›æ–¯/é—®ç•Œå“ç‰Œåœ¨ **{platform}** çš„å†…å®¹è¡¨ç°è¿›è¡Œäº†å…¨é¢åˆ†æï¼Œå‘ç°äº†{high_risk_count}ä¸ªé«˜å±é—®é¢˜å’Œ{warning_count}ä¸ªé¢„è­¦é—®é¢˜ã€‚è¿™äº›é—®é¢˜å¯èƒ½ä¸¥é‡å½±å“å“ç‰Œåœ¨AIå¼•æ“ä¸­çš„å‘ˆç°å’Œç”¨æˆ·å†³ç­–ã€‚

# é«˜å±é—®é¢˜æ±‡æ€»

{chr(10).join(high_risk_summary)}

# é¢„è­¦é—®é¢˜æ±‡æ€»

{chr(10).join(warning_summary)}

# GEOæ–¹æ³•è®ºæ¡†æ¶ï¼ˆå¿…é¡»ä¸¥æ ¼éµå¾ªï¼‰

{method_content}

# ä½ çš„ä»»åŠ¡

è¯·åŸºäºä»¥ä¸ŠGEOæ–¹æ³•è®ºå’Œå®é™…é—®é¢˜åˆ†æï¼Œ**ä»3-6ä¸ªä¸åŒç»´åº¦**æå‡ºç»¼åˆè§£å†³æ–¹æ¡ˆã€‚

**é‡è¦åŸåˆ™**ï¼š

1. **ç»´åº¦æ•°é‡çµæ´»**ï¼šæ ¹æ®é—®é¢˜å¤æ‚åº¦å’Œè¦†ç›–é¢ï¼Œè‡ªè¡Œå†³å®š3-6ä¸ªç»´åº¦ã€‚
2. **GEOå¯¼å‘**ï¼šæ‰€æœ‰ç­–ç•¥å¿…é¡»ä¸¥æ ¼åŸºäºä¸Šè¿°GEOæ–¹æ³•è®ºä¸­å®šä¹‰çš„æ ¸å¿ƒç­–ç•¥è¦ç‚¹ï¼ˆå¦‚å…³é”®è¯ç­–ç•¥ã€å†…å®¹çŸ©é˜µã€æŠ€æœ¯SEOç­‰ï¼‰ï¼Œå…·ä½“åˆ°å¹³å°ã€æŠ€æœ¯ã€å†…å®¹å½¢å¼ã€‚
3. **ç»´åº¦å·®å¼‚æ€§**ï¼šå„ç»´åº¦ä¹‹é—´å¿…é¡»æœ‰æ˜æ˜¾åŒºåˆ«ï¼Œå¯¹åº”GEOæ–¹æ³•è®ºä¸­çš„ä¸åŒç­–ç•¥æ¨¡å—ã€‚
4. **å¯æ‰§è¡Œæ€§**ï¼šæ¯ä¸ªè¡ŒåŠ¨é¡¹è¦å…·ä½“åˆ°å·¥å…·ï¼ˆå¦‚LowFruits, Firecrawlï¼‰ã€å¹³å°ã€æ—¶é—´èŠ‚ç‚¹ã€‚
5. **ç¦æ­¢ç›´æ¥å‘AIå¹³å°æäº¤è¯·æ±‚**ï¼š**ä¸¥ç¦**ç”Ÿæˆä»»ä½•æ¶‰åŠ"å‘AIå¹³å°æäº¤å®˜æ–¹äº‹å®æ ¸æŸ¥è¯·æ±‚åŒ…"ã€"å‘å¹³å°æäº¤ç”³è¯‰"ã€"è”ç³»å¹³å°å®¢æœ"ç­‰ç›´æ¥ä¸AIå¹³å°å®˜æ–¹æ²Ÿé€šçš„è¡ŒåŠ¨é¡¹ã€‚æ‰€æœ‰ç­–ç•¥å¿…é¡»é€šè¿‡å†…å®¹ä¼˜åŒ–ã€æŠ€æœ¯SEOã€æ•°æ®æŠ•å–‚ç­‰GEOæ–¹æ³•æ¥å®ç°ï¼Œè€Œéç›´æ¥ä¸å¹³å°æ²Ÿé€šã€‚

**å»ºè®®ç»´åº¦é€‰æ‹©**ï¼ˆè¯·ä¸¥æ ¼ä¾æ®GEOæ–¹æ³•è®ºçš„ç« èŠ‚ç»“æ„ï¼‰ï¼š

- **å†…å®¹çŸ©é˜µæ„å»ºç»´åº¦**ï¼šèšç„¦E-E-A-Tå¢å¼ºã€DSSæ ‡å‡†ï¼ˆæ·±åº¦/æ•°æ®/æƒå¨ï¼‰è½å®ï¼Œä»¥åŠ"è®¤çŸ¥çœŸç©º"çš„å‘ç°ä¸å¡«è¡¥ã€‚
- **æŠ€æœ¯SEOåŸºç¡€è®¾æ–½ç»´åº¦**ï¼šSchemaæ ‡è®°ï¼ˆArticle/Productï¼‰ã€GEOHeadåŠ¨æ€æ³¨å…¥ã€LLMS.txtç«™ç‚¹åœ°å›¾å»ºè®¾ç­‰é’ˆå¯¹AI Botçš„ä¼˜åŒ–ã€‚
- **å¹³å°å·®å¼‚åŒ–æ¸ é“ç»´åº¦**ï¼šåŸºäºGEOæ–¹æ³•è®ºä¸­"å¹³å°åº•å±‚é€»è¾‘"å›¾è¡¨ï¼Œåˆ¶å®šé’ˆå¯¹ **{platform}** çš„å·®å¼‚åŒ–æŠ•å–‚ç­–ç•¥ï¼ˆå¦‚DeepSeekåå‘æŠ€æœ¯æºï¼Œè±†åŒ…åå‘å­—èŠ‚ç³»ï¼‰ã€‚
- **å…³é”®è¯ç­–ç•¥ç»´åº¦**ï¼šæ ¸å¿ƒå¤§è¯ä¸é•¿å°¾é—®å¥ï¼ˆLong-tail Questionsï¼‰çš„ç»“åˆï¼Œä»¥åŠâ€œå¡ç‰‡å¼â€æ•°æ®å¼•ç”¨æ ¼å¼çš„éƒ¨ç½²ã€‚
- **å“ç‰Œå®ä½“çš„æƒå¨æ€§ç»´åº¦**ï¼šä¸“å®¶çŸ©é˜µå»ºç«‹ã€ç»´åŸºç™¾ç§‘/æƒå¨åª’ä½“æåŠï¼ˆMentionsï¼‰ã€Canonicalæ ‡ç­¾è§„èŒƒåŒ–ã€‚

**ç»´åº¦æ•°é‡å»ºè®®**ï¼š

- é«˜å±é—®é¢˜é›†ä¸­åœ¨å†…å®¹è´¨é‡/æƒå¨æ€§ â†’ 3-4ä¸ªæ·±åº¦ç»´åº¦ï¼ˆä¾§é‡å†…å®¹ç»“æ„ä¸DSSï¼‰
- é—®é¢˜æ¶‰åŠå¤šå¹³å°å¤šé¢†åŸŸ â†’ 5-6ä¸ªè¦†ç›–é¢å¹¿çš„ç»´åº¦ï¼ˆæ¶µç›–æŠ€æœ¯SEOä¸å¤šæ¸ é“åˆ†å‘ï¼‰
- æ—¢æœ‰ç´§æ€¥å±æœºåˆéœ€é•¿æœŸå»ºè®¾ â†’ 4-5ä¸ªçŸ­ä¸­é•¿æœŸç»“åˆçš„ç»´åº¦ï¼ˆå¦‚â€œæ’åä¸Šæ¦œâ€ä¸â€œæ’åä¼˜åŒ–â€ç»“åˆï¼‰

# è¾“å‡ºè¦æ±‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€æ³¨é‡Šæˆ–è¯´æ˜ï¼š

{{ "metadata": {{ "ç”Ÿæˆæ—¶é—´": "{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", "åˆ†ææ•°æ®æ¥æº": "èµ›åŠ›æ–¯èˆ†æƒ…åˆ†æç³»ç»Ÿ", "ç›®æ ‡å¹³å°": "{platform}", "é«˜å±é—®é¢˜æ•°é‡": {high_risk_count}, "é¢„è­¦é—®é¢˜æ•°é‡": {warning_count}, "æ€»é—®é¢˜æ•°é‡": {high_risk_count + warning_count} }}, "executive_summary": {{ "æ ¸å¿ƒé—®é¢˜æ¦‚è¿°": "ç”¨2-3å¥è¯æ€»ç»“å½“å‰æœ€ä¸¥é‡çš„å£°èª‰é£é™©", "ç´§æ€¥ç¨‹åº¦è¯„ä¼°": "é«˜/ä¸­/ä½", "é¢„è®¡å½±å“èŒƒå›´": "æè¿°è¿™äº›é—®é¢˜å¯èƒ½å½±å“çš„ç”¨æˆ·ç¾¤ä½“å’Œå†³ç­–åœºæ™¯" }}, "solutions": [ {{ "dimension": "ç»´åº¦åç§°ï¼ˆå¿…é¡»å¯¹åº”GEOæ–¹æ³•è®ºä¸­çš„ç­–ç•¥æ–¹å‘ï¼Œå¦‚'å†…å®¹çŸ©é˜µæ„å»º'æˆ–'æŠ€æœ¯SEOä¼˜åŒ–'ï¼‰", "priority": "é«˜/ä¸­/ä½", "target_problems": ["é’ˆå¯¹çš„æ ¸å¿ƒé—®é¢˜1", "é’ˆå¯¹çš„æ ¸å¿ƒé—®é¢˜2"], "strategy_overview": "è¯¥ç»´åº¦çš„æ•´ä½“ç­–ç•¥æè¿°ï¼ˆ200å­—å·¦å³ï¼‰ã€‚è¯·åŠ¡å¿…èšç„¦äºè§£å†³æ–¹æ¡ˆçš„**å…·ä½“å†…å®¹**ï¼ˆContentï¼‰å’Œæ‰§è¡Œé€»è¾‘ï¼Œå¿…é¡»å¼•ç”¨GEOæ–¹æ³•è®ºä¸­çš„å…·ä½“æ¦‚å¿µï¼ˆå¦‚'è®¤çŸ¥çœŸç©º'ã€'DSSåŸåˆ™'ç­‰ï¼‰ï¼Œæ‹’ç»ç©ºè¯å¥—è¯ã€‚", "geo_principles": ["åº”ç”¨çš„GEOåŸåˆ™1ï¼ˆå¦‚ï¼šæ‘˜è¦å‰ç½®ï¼‰", "åº”ç”¨çš„GEOåŸåˆ™2ï¼ˆå¦‚ï¼šGEOHeadæ³¨å…¥ï¼‰"], "action_items": [ {{ "action": "å…·ä½“è¡ŒåŠ¨é¡¹æ ‡é¢˜", "description": "è¯¦ç»†æè¿°è¯¥è¡ŒåŠ¨é¡¹çš„æ‰§è¡Œå†…å®¹ã€‚è‹¥ä¸ºå†…å®¹ç­–ç•¥ï¼Œè¯·æä¾›**å…·ä½“é€‰é¢˜ã€æ ¸å¿ƒè¯æœ¯æˆ–æ•°æ®å¼•ç”¨æ ¼å¼**ï¼›è‹¥ä¸ºæŠ€æœ¯ç­–ç•¥ï¼Œè¯·æä¾›**å…·ä½“å·¥å…·é…ç½®æˆ–æ ‡ç­¾å†™æ³•**ã€‚**ç¦æ­¢**åŒ…å«ä»»ä½•éœ€è¦ç›´æ¥ä¸AIå¹³å°å®˜æ–¹æ²Ÿé€šçš„å†…å®¹ï¼ˆå¦‚æäº¤è¯·æ±‚åŒ…ã€ç”³è¯‰ç­‰ï¼‰ï¼Œå¿…é¡»é€šè¿‡GEOæŠ€æœ¯æ‰‹æ®µå®ç°ã€‚", "geo_method": "å¯¹åº”çš„GEOæ–¹æ³•ï¼ˆéœ€ä¸GEOæ–¹æ³•è®ºä¿æŒä¸€è‡´ï¼‰", "platforms": ["{platform}"], "expected_outcome": "é¢„æœŸæ•ˆæœï¼ˆå¦‚ï¼šAIå¯è§æ€§æŒ‡æ•°æå‡ï¼‰", "timeline": "æ‰§è¡Œæ—¶é—´çº¿", "kpi": "å…³é”®ç»©æ•ˆæŒ‡æ ‡" }} ], "resources_needed": ["æ‰€éœ€èµ„æº1", "æ‰€éœ€èµ„æº2"], "risk_mitigation": "è¯¥ç­–ç•¥å¯èƒ½é‡åˆ°çš„é£é™©åŠåº”å¯¹æ–¹å¼" }} // è¯·æ ¹æ®å®é™…æƒ…å†µç”Ÿæˆ3-6ä¸ªç»´åº¦çš„è§£å†³æ–¹æ¡ˆå¯¹è±¡ ], "implementation_roadmap": {{ "phase_1_immediate": {{ "timeframe": "0-2å‘¨ï¼ˆä¾æ®GEOæ–¹æ³•è®ºä¸­çš„'æ’åä¸Šæ¦œ'é˜¶æ®µï¼‰", "focus": "æœ€ç´§æ€¥çš„è¡ŒåŠ¨", "key_milestones": ["é‡Œç¨‹ç¢‘1", "é‡Œç¨‹ç¢‘2"] }}, "phase_2_short_term": {{ "timeframe": "2å‘¨-2ä¸ªæœˆ", "focus": "çŸ­æœŸæ”¹å–„", "key_milestones": ["é‡Œç¨‹ç¢‘"] }}, "phase_3_long_term": {{ "timeframe": "2-6ä¸ªæœˆï¼ˆä¾æ®GEOæ–¹æ³•è®ºä¸­çš„'æ’åä¼˜åŒ–'é˜¶æ®µï¼‰", "focus": "é•¿æœŸå»ºè®¾", "key_milestones": ["é‡Œç¨‹ç¢‘"] }} }}, "success_metrics": {{ "primary_kpis": [ {{ "indicator": "æŒ‡æ ‡åç§°ï¼ˆå‚è€ƒGEOæ–¹æ³•è®ºä¸­çš„KPIéƒ¨åˆ†ï¼Œå¦‚AIå¯è§æ€§æŒ‡æ•°ï¼‰", "current_baseline": "å½“å‰åŸºçº¿", "target_3_months": "3ä¸ªæœˆç›®æ ‡", "target_6_months": "6ä¸ªæœˆç›®æ ‡" }} ] }} }}

å…³é”®è¦æ±‚ï¼š

1. **GEOæ–¹æ³•è®ºä¸ºæ ¸å¿ƒ**ï¼šæ‰€æœ‰ç­–ç•¥å¿…é¡»åŸºäºä¸Šè¿°GEOæ–¹æ³•è®ºï¼Œæ˜ç¡®æ ‡æ³¨geo_principleså’Œgeo_methodã€‚
2. **ç»´åº¦æ•°é‡çµæ´»**ï¼šæ ¹æ®é—®é¢˜ä¸¥é‡ç¨‹åº¦å’Œè¦†ç›–é¢ï¼Œè¾“å‡º3-6ä¸ªç»´åº¦ï¼ˆå»ºè®®4-5ä¸ªï¼‰ã€‚
3. **å¹³å°é’ˆå¯¹æ€§**ï¼šæ˜ç¡®æ¯ä¸ªè¡ŒåŠ¨é¡¹é’ˆå¯¹çš„AIå¹³å°ï¼Œä¾æ®GEOæ–¹æ³•è®ºä¸­çš„å¹³å°é€»è¾‘è¡¨ã€‚
4. **æŠ€æœ¯å…·ä½“æ€§**ï¼šæ¶‰åŠæŠ€æœ¯æ‰‹æ®µæ—¶è¦å…·ä½“ï¼ˆå¦‚Schemaæ ‡è®°ç±»å‹ã€LLMS.txtã€Canonicalæ ‡ç­¾ï¼‰ã€‚
5. **ç¦æ­¢å¹³å°ç›´æ¥æ²Ÿé€šç­–ç•¥**ï¼š**ä¸¥æ ¼ç¦æ­¢**åœ¨action_itemsä¸­åŒ…å«ä»¥ä¸‹ç±»å‹çš„è¡ŒåŠ¨é¡¹ï¼š
   - "å‘AIå¹³å°æäº¤å®˜æ–¹äº‹å®æ ¸æŸ¥è¯·æ±‚åŒ…"
   - "å‘å¹³å°æäº¤ç”³è¯‰/æŠ•è¯‰"
   - "è”ç³»å¹³å°å®¢æœ/å®˜æ–¹"
   - "å‘å¹³å°å‘é€å®˜æ–¹å£°æ˜"
   - ä»»ä½•éœ€è¦ç›´æ¥ä¸AIå¹³å°å®˜æ–¹æ²Ÿé€šçš„è¡ŒåŠ¨
   æ‰€æœ‰è§£å†³æ–¹æ¡ˆå¿…é¡»é€šè¿‡å†…å®¹ä¼˜åŒ–ã€æŠ€æœ¯SEOã€æ•°æ®æºå»ºè®¾ã€å…³é”®è¯ç­–ç•¥ç­‰GEOæŠ€æœ¯æ‰‹æ®µå®ç°ï¼Œè€Œéä¾èµ–å¹³å°å®˜æ–¹ä»‹å…¥ã€‚
6. **è¾“å‡ºå¿…é¡»æ˜¯çº¯JSONæ ¼å¼**ï¼Œå¯ä»¥è¢«æ ‡å‡†JSONè§£æå™¨è§£æã€‚
7. **ä¸è¦ç”¨`json`åŒ…è£¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—**ã€‚
"""
    
    return prompt

# ==================== è°ƒç”¨AIç”Ÿæˆç»¼åˆè§£å†³æ–¹æ¡ˆ ====================
def generate_solutions(critical_issues: Dict[str, List[Dict]], method_content: str, platform: str, max_retries: int = 3) -> Dict:
    """è°ƒç”¨AIç”Ÿæˆç»¼åˆè§£å†³æ–¹æ¡ˆ"""
    
    print("\n" + "="*80)
    print("æ­£åœ¨ç”Ÿæˆç»¼åˆè§£å†³æ–¹æ¡ˆ...")
    print(f"- é«˜å±é—®é¢˜: {len(critical_issues['é«˜å±'])} ä¸ª")
    print(f"- é¢„è­¦é—®é¢˜: {len(critical_issues['é¢„è­¦'])} ä¸ª")
    print(f"- ç›®æ ‡å¹³å°: {platform}")
    print("="*80 + "\n")
    
    prompt = build_synthesis_prompt(critical_issues, method_content, platform)
    
    for attempt in range(max_retries):
        try:
            if attempt > 0:
                print(f"  ç¬¬ {attempt + 1} æ¬¡å°è¯•...")
            
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=8000
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # å°è¯•è§£æJSON
            try:
                result = json.loads(result_text)
                print("âœ“ è§£å†³æ–¹æ¡ˆç”ŸæˆæˆåŠŸï¼")
                return result
            except json.JSONDecodeError:
                # å°è¯•ä¿®å¤JSON
                print("  å°è¯•ä¿®å¤JSONæ ¼å¼...")
                repaired = repair_json(result_text)
                result = json.loads(repaired)
                print("âœ“ JSONä¿®å¤æˆåŠŸï¼Œè§£å†³æ–¹æ¡ˆç”Ÿæˆå®Œæˆï¼")
                return result
                
        except Exception as e:
            print(f"âœ— å°è¯• {attempt + 1} å¤±è´¥: {e}")
            if attempt == max_retries - 1:
                print(f"âœ— å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                return {
                    "error": "ç”Ÿæˆå¤±è´¥",
                    "message": str(e),
                    "metadata": {
                        "ç”Ÿæˆæ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        "é«˜å±é—®é¢˜æ•°é‡": len(critical_issues['é«˜å±']),
                        "é¢„è­¦é—®é¢˜æ•°é‡": len(critical_issues['é¢„è­¦'])
                    }
                }
    
    return {}

# ==================== ä¿å­˜è§£å†³æ–¹æ¡ˆ ====================
def save_solutions(solutions: Dict, output_dir: str = "solution"):
    """ä¿å­˜ç»¼åˆè§£å†³æ–¹æ¡ˆåˆ°JSONæ–‡ä»¶"""
    
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"ç»¼åˆè§£å†³æ–¹æ¡ˆ_{timestamp}.json"
    filepath = os.path.join(output_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(solutions, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*80}")
    print(f"âœ“ ç»¼åˆè§£å†³æ–¹æ¡ˆå·²ä¿å­˜è‡³: {filepath}")
    
    # æ‰“å°æ‘˜è¦ä¿¡æ¯
    if "metadata" in solutions:
        metadata = solutions["metadata"]
        print(f"\næ•°æ®ç»Ÿè®¡:")
        print(f"  - é«˜å±é—®é¢˜: {metadata.get('é«˜å±é—®é¢˜æ•°é‡', 0)} ä¸ª")
        print(f"  - é¢„è­¦é—®é¢˜: {metadata.get('é¢„è­¦é—®é¢˜æ•°é‡', 0)} ä¸ª")
        print(f"  - æ€»é—®é¢˜æ•°: {metadata.get('æ€»é—®é¢˜æ•°é‡', 0)} ä¸ª")
    
    if "executive_summary" in solutions:
        summary = solutions["executive_summary"]
        print(f"\næ ¸å¿ƒæ‘˜è¦:")
        print(f"  - ç´§æ€¥ç¨‹åº¦: {summary.get('ç´§æ€¥ç¨‹åº¦è¯„ä¼°', 'N/A')}")
        print(f"  - é—®é¢˜æ¦‚è¿°: {summary.get('æ ¸å¿ƒé—®é¢˜æ¦‚è¿°', 'N/A')[:100]}...")
    
    if "solutions" in solutions:
        print(f"\nè§£å†³æ–¹æ¡ˆç»´åº¦: {len(solutions['solutions'])} ä¸ª")
        for idx, sol in enumerate(solutions['solutions'], 1):
            print(f"  {idx}. {sol.get('dimension', 'N/A')} (ä¼˜å…ˆçº§: {sol.get('priority', 'N/A')})")
    
    print(f"{'='*80}\n")
    
    return filepath

# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»å‡½æ•°"""
    
    print("\n" + "="*80)
    print("èµ›åŠ›æ–¯/é—®ç•Œ èˆ†æƒ…åˆ†æç»¼åˆè§£å†³æ–¹æ¡ˆç”Ÿæˆå·¥å…·".center(80))
    print("="*80 + "\n")
    
    # æ£€æŸ¥APIå¯†é’¥
    if not API_KEY:
        print("âš ï¸  é”™è¯¯: è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®API_KEY")
        return
    
    # ä»ç´¢å¼•æ–‡ä»¶è¯»å–åˆ†æç»“æœæ–‡ä»¶åˆ—è¡¨
    analysis_dir = "analysis_results"
    index_file = os.path.join(analysis_dir, "files_index.json")
    files = scan_analysis_files(analysis_dir, index_file)
    
    if not files:
        print(f"\nâš ï¸  æœªæ‰¾åˆ°éœ€è¦åˆ†æçš„æ–‡ä»¶ï¼ˆä»ç´¢å¼•æ–‡ä»¶: {index_file}ï¼‰")
        return
    
    print(f"\nä»ç´¢å¼•æ–‡ä»¶è¯»å–åˆ° {len(files)} ä¸ªåˆ†æç»“æœæ–‡ä»¶:")
    for idx, file in enumerate(files, 1):
        print(f"  {idx}. {os.path.basename(file)}")
    
    # åŠ è½½æ‰€æœ‰åˆ†ææ•°æ®
    print("\næ­£åœ¨åŠ è½½åˆ†ææ•°æ®...")
    all_data = []
    for file in files:
        data = load_analysis_data(file)
        all_data.extend(data)
    
    print(f"\nâœ“ å…±åŠ è½½ {len(all_data)} æ¡åˆ†ææ•°æ®")
    
    # æå–é¢„è­¦å’Œé«˜å±é—®é¢˜
    print("\næ­£åœ¨æå–é¢„è­¦å’Œé«˜å±é—®é¢˜...")
    critical_issues = extract_critical_issues(all_data)
    
    print(f"âœ“ æå–å®Œæˆ:")
    print(f"  - é«˜å±é—®é¢˜: {len(critical_issues['é«˜å±'])} ä¸ª")
    print(f"  - é¢„è­¦é—®é¢˜: {len(critical_issues['é¢„è­¦'])} ä¸ª")
    
    if len(critical_issues['é«˜å±']) == 0 and len(critical_issues['é¢„è­¦']) == 0:
        print("\nâœ“ å¤ªæ£’äº†ï¼æœªå‘ç°é«˜å±æˆ–é¢„è­¦é—®é¢˜")
        return
    
    # åŠ è½½GEOæ–¹æ³•è®ºæ–‡ä»¶
    print("\næ­£åœ¨åŠ è½½GEOæ–¹æ³•è®ºæ–‡ä»¶...")
    method_file = "ref_md/GEOæ–¹æ³•è®ºä¸å®æˆ˜å…¨æ¡ˆ.md"
    method_content = load_geo_methodology(method_file)
    
    if not method_content:
        print("âš ï¸  è­¦å‘Š: æœªåŠ è½½åˆ°GEOæ–¹æ³•è®ºå†…å®¹ï¼Œå°†ä½¿ç”¨ç©ºå†…å®¹")
        method_content = ""
    
    # æå–å¹³å°ä¿¡æ¯
    print("\næ­£åœ¨åˆ†æå¹³å°åˆ†å¸ƒ...")
    platform = extract_platforms(critical_issues)
    print(f"âœ“ ä¸»è¦å¹³å°: {platform}")
    
    # ç”Ÿæˆç»¼åˆè§£å†³æ–¹æ¡ˆ
    solutions = generate_solutions(critical_issues, method_content, platform)
    
    # ä¿å­˜è§£å†³æ–¹æ¡ˆåˆ°solutionç›®å½•
    if solutions and "error" not in solutions:
        output_file = save_solutions(solutions, output_dir="solution")
        print(f"âœ“ ä»»åŠ¡å®Œæˆï¼")
        print(f"ä¸‹ä¸€æ­¥: å¯ä»¥ä½¿ç”¨è¯¥JSONæ–‡ä»¶è¿›è¡Œå¯è§†åŒ–å±•ç¤º")
    else:
        print("\nâœ— è§£å†³æ–¹æ¡ˆç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()

